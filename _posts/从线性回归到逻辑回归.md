---

title: '从线性回归到逻辑回归'
date: 2019-04-10
permalink: /posts/2014/04/blog-post-1/
---



**回归分析(Regression analysis)**是统计学上分析变量之间关系的一系列方法的统称。更加具体的说，回归分析帮我们了解任意一个自变量变化时因变量的变化情况。回归预测值是连续的，最简单的例子就是预测商品价值、预测股票等等例子。

### 线性回归

**线性回归**则是**回归分析**中最为简单的一种方法，顾名思义，它的自变量和因变量之间是线性关系。假如我们有一堆数据如下图所示：

```python
import numpy as np
import matplotlib.pyplot as plt

def generate_data(num_data,slop,offset):
    x = np.linspace(-5,5,num_data)
    y = slop*x + offset + np.random.normal(loc=0,scale = 1.0, size = num_data)
    return x, y

if __name__ == '__main__':
    #first generate data
    x, y = generate_data(500,0.8,8)
	plt.scatter(x,y,marker='x')
	plt.show()
```

![gen_data](/home/jie/Documents/jieyang1987.github.io/files/linReg_and_logReg/gen_data.png)

从最为简单的单个变量入手，我们可以认为：
$$
\hat{y_i} = mx_i+ b
$$


其中$x_i$为自变量，$\hat{y_i}$为线性回归预测得到的值。很显然，上面的表达式为二维平面上的一条直线。为了选择合适的$m$和$b$值来实现预测值，我们引入损失函数(cost function):
$$
(\hat{y_i}- y_i)
$$
其中，${y_i}$为$x_i$对应的真实值。如果存在$n$个样本，那么针对所有的样本的损失函数则为：
$$
\frac{1}{2n}\sum_{i=0}^{n-1}(\hat{y_i}-y_i)^2
$$
上式中$\frac{1}{2}$仅仅是为了后续计算方便。显然，损失函数越小，代表着模型越准确，因此寻找最合适$m$和$b$的过程就是最小化上述损失函数的过程。根据大学最基本的微积分课程，寻找极值点的方法是寻找函数偏导数为零的点，为此，根据求导法则，我们进行如下计算：

$$
\frac{\partial f}{\partial m} = \sum_{i=0}^{n-1}(m x_i + b -y_i)\times x_i = m\sum_{i=0}^{n-1}{x_i}^2+ b\sum_{i=0}^{n-1}x_i -\sum_{i=0}^{n-1}{x_i}{y_i}  \\
 \frac{\partial f}{\partial b}  = \sum_{i=0}^{n-1}(m x_i + b -y_i) = m\sum_{i=0}^{n-1}x_i + nb - \sum_{i=0}^{n-1}y_i
$$

为了方便书写，我们可以定义$\bar{x}\times n = {\sum_{i=0}^{n-1}x_i}$ ，以及$\bar{y}\times n = {\sum_{i=0}^{n-1}y_i}$，直接将$\sum_{i=0}^{n-1}$写成$\sum$同时，我们令上面的式子都为零，这样可以进一步的得到：
$$
\begin{align}
& \frac{\partial f}{\partial m} = m \sum {x_i}^2 + n\bar{x}b-\sum{x_i}{y_i} = 0 \\
& \frac{\partial f}{\partial b} = mn\bar{x} + nb - n\bar{y} = 0
\end{align}
$$
很显然可以推得：$b = \bar{y} - m\bar{x}$ ，将其带入上面得第一个式子，可以得到：
$$
\begin{align}
& m\sum{x_i}^2 + n\bar{x}(\bar{y}-m\bar{x})-\sum{x_i}{y_i} = 0 \\
&  m(\sum{x_i}^2-n\bar{x}^2) = \sum{x_i}{y_i}- n\bar{x}\bar{y}\\
&  m = \frac{\sum{x_i}{y_i}-n\bar{x}\bar{y}}{\sum{x_i}^2-n\bar{x}^2}
\end{align}
$$
进一步的化简[[1]](#Appendix)，我们还可以进一步的得到：
$$
m = \frac{\sum{(x_i-\bar{x})(y_i-\bar{y})}}{\sum(x_i-\bar{x})^2}
$$
这样，$m$和$b$的值都通过求导的方式的出来了。因为我们已经求得了最终得$m$和$b$表达式，因此可以使用如下得`Python`代码直接求取：

```python
def least_squares(x,y):
    xbar = np.average(x)
    ybar = np.average(y)
    m = np.sum((x-xbar)*(y-ybar))/np.sum((x-xbar)**2)
    b= ybar-m*xbar
    return m, b
```

上述方法其实就是**最小二乘法**。除此以外，我们还使用**梯度下降法**来求解，其实原理类似，都是希望得到最小化损失函数得位置，最小二乘法采用了直接计算得方式，梯度下降法则是慢慢得寻找到这一是函数导数为零得点：

需要不断的更新$m$和$b$参数：
$$
m := m- \alpha \frac{\partial f}{\partial m} \\
b := b - \alpha \frac{\partial f}{\partial b}
$$
其中， ${\partial f}/{\partial m}$和${\partial f}/{\partial b}$的值前面已经计算出来了，我们通过使用`Python`代码实现上述梯度下降的过程：

```python
def gradient_decent(x,y,steps,alpha):
    #give m and b random number first
    m, b = np.random.rand(2)
    n = x.shape[0]
    for i in range(steps):
        partial_m = np.sum((m*x+b-y)*x) / n
        partial_b = np.sum(m*x+b-y) / n
        m -= alpha*partial_m
        b -= alpha*partial_b
    return m, b
```

通过使用上述的最小二乘法和梯度下降法，我们已经可以较为准确的预测$y$值了。为了验证最小二乘法和梯度下降的结果，我们使用下面的函数进行计算并可视化。

```Python
if __name__ == '__main__':
    #set some parameters
    num_data = 500
    slope = 0.8
    offset = 8
    num_steps = 100
    alpha = 0.1
    #first generate data
    x, y = generate_data(num_data,slope,offset)
    
    #first visualize the data 
	plt.scatter(x,y,marker='x')
	plt.show()
    
    #comput and plot the m and b computed from the least_squares method
    m, b = least_squares(x,y)
    
	#comput and plot the m and b computed with gradient descent
    m, b = gradient_decent(x,y,num_steps,alpha)
```



当然，我们完全可以把自变量的数量由有两个($m$和$b$)，推广到更多，也就是如下所示：
$$
\hat{y} =\theta_0 + \theta_1 x_1 +  \theta_2 x_2 + ...+ \theta_{n} x_{n}
$$

目前存在$n$个自变量，考虑到offset $\theta_0$，为了后续计算表达方便，我们增加一个固定量$x_0 = 1$，从而将上面的公式重新写为：
$$
\hat{y} =\theta_0 x_0 + \theta_1 x_1 +  \theta_2 x_2 + ...+ \theta_{n} x_{n}
$$

如果，分别将参数和变量都写为向量形式的话，我们得到：


$$
\theta = \left( \begin{matrix}

\theta_0 \\
\theta_1 \\
...\\
\theta_n
\end{matrix}
\right), \ \ \ x =  \left( \begin{matrix}

x_0 \\
x_1 \\
...\\
x_n
\end{matrix}
\right)
$$

那么我们可以重写$\hat{y}(x)$的表达式如下：


$$
\hat{y}^{(i)} = \theta^T x^{(i)}
$$

上标表示第$i$个样本。我们使用与之前相同的损失函数，同时使用矩阵表示[[2]](#Appendix)：
$$
\frac{1}{2}(X\theta-y)^T(X\theta-y) 
$$
展开上式，并令其为零，我们可以进一步的得到：
$$
= ((X\theta)^T-y^T)(X\theta-y)\\
= (X\theta)^TX\theta-(X\theta)^Ty-
$$


上述实现在Python下的代码如下：

```python
def normal_equation(x, y):
    
    
```

同样的，我们可以也使用梯度下降法对这些参数进行更新，首先我们的损失函数仍旧为：
$$
\frac{1}{2n}\sum(\hat{y}^{(i)}-y^{(i)})^2 = \frac{1}{2n}\sum_{i=1}^{n}(\theta_0 x_0^{(i)} + \theta_1 x_1^{(i)} +  \theta_2 x_2^{(i)} + ...+ \theta_{n} x_{n}^{(i)} -y^{(i)})^2
$$
这里用上标来表示第$i$个样本。使用梯度下降法，求解对$\theta_i$的导数
$$
\frac{\partial f}{\partial \theta_0} = \frac{1}{n}\sum(\theta_0 x_0^{(i)} + \theta_1 x_1^{(i)} +  \theta_2 x_2^{(i)} + ...+ \theta_{n} x_{n}^{(i)} - y^{(i)})x_0^{(i)}  \\
\frac{\partial f}{\partial \theta_1} = \frac{1}{n}\sum(\theta_0 x_0^{(i)} + \theta_1 x_1^{(i)} +  \theta_2 x_2^{(i)} + ...+ \theta_{n} x_{n}^{(i)}- y^{(i)})x_1^{(i)} \\
...\\
\frac{\partial f}{\partial \theta_n} = \frac{1}{n}\sum(\theta_0 x_0^{(i)} + \theta_1 x_1^{(i)} +  \theta_2 x_2^{(i)} + ...+ \theta_{n} x_{n}^{(i)}- y^{(i)})x_n^{(i)}
$$

进一步的：
$$
\theta_0 := \theta_0 - \alpha\frac{\partial f}{\partial \theta_0} \\ 
\theta_1 := \theta_1 - \alpha\frac{\partial f}{\partial \theta_1} \\ 
... \\
\theta_n := \theta_n - \alpha\frac{\partial f}{\partial \theta_n} \\
$$
其代码实现如下:

```python
def gradient_decent_multiple_two_loop(x, y, num_steps,alpha):
    k, n = x.shape 
    for i in range(num_steps):
        for j in range(n):
        theta_i = theta_i - alpha*np.sum( theta*x(i)- y(i)  )
```

不难发现，其实上面的代码

```python
def gradient_decent_multiple_one_loop
```







### 逻辑回归/分类

除了预测直接预测股票的值以外，还可以预测股票的涨或者跌。这类问题与回归问题不同，它需要的答案是Yes or No。逻辑回归其实是逻辑分类，只不过因为一些历史原因，被叫做了逻辑回归而已。我们同样从简单的二分类开始。首先来生成点数据：

```python

```

同上面的图中很容易直观的感觉到，一类数据在"下"，一类数据在"上"。分割它们的任意一条线都可以具有分类的能力：
$$
\begin{equation}
class(x) = \begin{cases}
+1 \ \ if \ \ \ {f}(x) \ge 0\\
-1 \ \ if \ \ \ {f}(x) \lt 0
\end{cases}
\end{equation}
$$
为了正确的求得这样得函数$f=mx+b$，我们仍然可以采用损失函数得办法。我们首先定义一个最为简答得损失函数：
$$
L_{01}(f(x_i)y_i) = \amalg (f(x_i)y_i \le 0)
$$
其中$\amalg(f(x_i)y_i \le 0)$表示，如果$f(x_i)y_i$的值小于等于零，那么式子取值为一，反之取值为零。这一点，不难理解，如果分类正确的话，$f(x_i)$和$y_i$是同号的，这样$f(x_i)y_i$大于零，式子取值为零，没有增加损失。反之如果分类错误的话，$f(x_i)$和$y_i$将反号，这样$f(x_i)y_i$的值将小于零，式子取值为一，增加了损失。其实这个表达式就是计算了当前模型分类错误的样本数量，是目前较为符合逻辑的一种选择。我们可以用Python来可视化上述函数：

```python

```

但是上述损失函数非凸，这就导致我们不好使用梯度下降了。其实理解起来很简单，如上图的函数，假如现在的参数可以正确推断$k$个样本，如果我们稍微改变一下，很显然，也可以保持正确推断$k$个样本，这样的话，损失函数没有改变，我们无法通过梯度下降来继续寻找更好的参数了。考虑到上述问题，我们可以将损失函数做一定的修正，比如说采用平方损失函数：
$$
L_2(f(x_i)y_i) = (f(x_i)y_i - 1)^2
$$
该损失函数如下图所示：





但是，该损失函数会产生如下很多问题，

- 

- 



为了解决这个问题，我们可以采用`hinge loss ` 损失函数，如下所示：
$$
max(0,1-f(x_i)y_i)
$$
这个损失函数看起来像这样子：









针对分类问题，我们也可以接受这样的答案："股市明天涨的概率是80%，跌的概率是20%"，如果我们要得到这样的结果的话，那么我们的输出就会是一个介于$[0,1]$之间的值。如果$f(x)$已经具备正确分类的能力，其的输出范围是远远超过$0$和$1$的（如下图所示），为了实现将输出值"挤压"到$[0,1]$之间，可以使用`Sigmoid curve`。这个函数又叫做`Sigmoid function`、`Logistic function`， 因为这个函数的使用，这种方法就叫做逻辑回归`logistic regression`。
$$
S(f(x)) = \frac{1}{1+e^{-f(x)}}
$$
我们使用下面的代码画出`Sigmoid`函数：

```Python

```

从上面的示意图可以看出，无论$f(x)$的取值范围如何，都会被挤压"到$[0,1]$之间。但是到目前为止，我们只假设了分类函数的形式。针对这类答案，我们还需要进一步定义合适的损失函数。







交叉熵损失函数：
$$
\begin{equation}c(x) = \begin{cases}+1 \ \ if \ \ \ {f}(x) \ge 0\\-1 \ \ if \ \ \ {f}(x) \lt 0\end{cases}\end{equation}
$$



### Softmax 







### Appendix

[1] 该简化过程使用了求和性质，具体证明方法如下：
$$
\begin{align}
&\sum(x_i-\bar{x})(y_i-\bar{y}) = \\
& \sum(x_iy_i - x_i\bar{y}-\bar{x}y_i+\bar{x}\bar{y}) = \\
&  \sum{x_iy_i}-n\bar{x}\bar{y}-n\bar{x}\bar{y}+n\bar{x}\bar{y} = \sum{x_iy_i-n\bar{x}\bar{y}} \\
\\
& \sum{(x_i-\bar{x})^2} = \\
& \sum{{x_i}^2}-2\sum{x_i\bar{x}}+\sum{\bar{x}}^2 = \\
& \sum{x_i}^2- 2n\bar{x}^2+n\bar{x}^2 = \sum{x_i}^2 - n\bar{x}^2 
\end{align}
$$

 [2]关于Matrix calculus：