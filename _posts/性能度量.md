---
title: '性能度量'
date: 2013-08-14
permalink: /posts/2013/08/blog-post-2/
---





一个机器学习模型的好坏需要统一的指标去评价，这样子比来比去，大家才能说哪个模型好，哪个模型没有那么好。对于分类和回归两类任务，分别有各自不同的评价标准，因为当前的工作主要涉及分类的评价，所以这里仅仅介绍分类的性能度量标准。



##### 混淆矩阵（confusion matrix）

对于一个二分类问题，根据实际情况和预测结果，可能出现如下四种情况，我们将其写入一个如下混淆矩阵:

|              |          | **事实为正**       |    **事实为负**    |
| ------------ | :------: | :----------------- | :----------------: |
|              |          | True               |      Negative      |
| **预测为真** |   True   | True Positive, TP  | False Positive, FP |
| **预测为负** | Negative | False Negative, FN | True Negative, TN  |



#####Accuracy

根据混淆矩阵，我们可以很容易的得到准确率(**Accuracy**)的定义，也就是**预测正确的样本**占**总样本**的百分比：
$$
Accuracy = \frac{(TP + TN)}{TP + TN + FP + FN}
$$
虽然上面的公式可以判断总的正确率，但是在样本极不均衡的情况下，上述准确率并不能反映一个模型的好坏。比如在一个总样本中，正样本占95%，负样本仅仅占5%，样本极其不平衡的。我们只需要将全部样本预测为正样本即可得到95%的高准确率，但实际上我们只是无脑操作而已。这个例子很能说明问题，也就是在样本不平衡的情况下，高准确率并不能说明什么问题。



##### Precision & Recall

精准率(**Precision**)是指所有被预测为正的样本中实际为正的样本的概率，那么其计算公式很简单，因为为：
$$
Precision =\frac{TP}{TP+FP}
$$
召回率(**Recall**)是指实际为正的样本中被预测为正样本的概率，其公式如下：
$$
Recall = \frac{TP}{TP+FN}
$$
Precision和Recall有点trade-off的感觉，往往存在相互制约的关系。如果你想要Recall everything，那么很显然你需要提高模型的泛化程度，也就是会引入更多的False positive，从而降低了Precision。有一种宁可错杀一千，不可放过一个的感觉。如果你想要更高的Precision，显然模型就会变得更加严格，这样就会出现更多的漏网之鱼，也就是降低了Recall。有一种宁可放过一千，不可错杀一个的感觉。在现实生活中，不用应用场景下的模型具有不同的对于Precision或者Recall的偏好。比如，在购物平台中搜索某个关键字，我们希望搜索结果都是匹配的，并且尽量不要出现无关的搜索结果，因为太多无关的结果，可能用户就要换平台了，这种情况就是要提高模型的Precision，让搜索出来的结果都是对的。然而，对于网络借贷平台，以个人违约率为例，相对信用用户，我们更关心坏用户，不能错放过任何一个坏用户。因为如果我们过多的将坏用户当成好用户，这样后续可能发生的违约金额会远超过好用户偿还的借贷利息金额，造成严重偿失。这种情况就是要尽量的将坏用户都找到，也就是提高模型的Recall。

#####Precision-recall curve

逻辑回归的输出是一个0到1之间的概率数字，为了进行二分类，我们想定义一个阈值，高于阈值认为为正样本，反之则为负样本。因此，对某个阈值为$t$，我们可以得到相应的一对Precision和Recall。如果我们遍历0到1之间的所有值，那么我们就可以得到多对Precision和Recall，将这些点连起来，就得到了Precision-Recall曲线了。一个典型的Precision-Recall的图像如下所示，其中X轴为Recall，Y轴为Precision。

![Precision-Recall曲线示例](<https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/Line-Plot-of-Precision-Recall-Curve.png>)

我们可以看到，Recall越小的时候，Precision越高，随着Recall逐渐增加，Precision降低。这是符合我们刚刚的分析的。Precision-Recall曲线约陡越好，因为这样Recall和Precision都相对较高。如果数据中正样本和负样本的数量一样多，那么随机猜测得到的Precision就应该是0.5，也就是图中的虚线。如果数据中正负样本的比例不是1:1，而是2:8，那么这个虚线就会到0.2的位置。因为随机猜测的正确率从0.5变成了0.2。这也说明，Precision-Recall曲线会随着样本的分布不同产生较大的变化。



##### F1 Score

F1 Score是一个综合Precision和Recall的综合衡量指标，也就是二者的调和平均值(harmonic mean):
$$
F1 = 2 \times \frac{Prcision \times Recall}{Precision + Recall}
$$
当使用F1作为衡量指标的时候，就不用再分别考虑Precision和Recall，而是综合考虑这一个度量指标就可以了。



##### Sensitivity & Specificity

灵敏度(**Sensitivity**)，其实就是Recall，其实这个名字不难理解，总共有100个正样本，模型Recall了90个，这不正是模型“灵敏度”的表现吗？特异性(Specificity)与灵敏度其实度量的是同样的东西，只不过它关心的是负样本：
$$
Specificity = \frac{TN}{FP + TN}
$$
可以认为它是负样本的Recall，只是换了个名字。但是，我们更加关心正样本，所以需要查看有多少负样本被错误地预测为正样本。从而引出下面两个新的指标，真正率和假正率
$$
True\ Positive\ Rate (TPR) = Sensitivity = Recall = \frac{TP}{TP+FN}
$$

$$
False\ Positive\ Rate(FPR) = 1 - Specificity = \frac{FP}{FP + TN}
$$



从概率的角度来考虑的话，如果$X$是真实值，$Y$为预测值，那么：
$$
Recall = Sensitivity = P(Y = true | X = true) \\

Specificity = P(Y = negative | X= negative)
$$

从TPR和FPR的表达式可以看出，他们的分母其实是样本中总的**事实正样本**和**事实负样本**的数量。也就是说这两个指标是不受样本均衡性影响的。

> TPR describes how good the model is at predicting the positive class when the actual outcome is positive, FPR summarizes how often a positive class is predicted when the actual outcome is negative.




#####  Receiver operating characteristic curve  

接受者操作特征曲线(Receiver Operating Characteristic)是由上述Sensitivity和Specificity绘制而来，具体的，其X轴为1-Specificity​，也就是FPR，Y轴为Sensitivity，也就是TPR。 ROC曲线的具体绘制方法和Precision-Recall曲线类似，也是通过遍历所有阈值来绘制的。一个典型的ROC曲线如下所示：

![ROC曲线](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/06/Line-Plot-of-ROC-Curve.png)

我们可以看到，随着Sensitivity，也就是Recall的增加，FPR也逐渐提高。这是符合我们刚刚的分析的。ROC曲线也是越陡越好（比如Recall = 1，FPR = 0，这显然是理想状态）。如果我们随机的判断一个样本是正样本还是负样本，那么很显然我们应该得到0.5的FPR和0.5的TPR，这就是上图中的虚线所表示的意义。因为ROC的两个坐标都和样本均衡性无关，所以与Precision-Recall不同，其不会随着样本均衡性的变化而产生较大变化。下面的一个图(from [ROC graphs: Notes and practical considerations for researchers](http://www.blogspot.udec.ugto.saedsayad.com/docs/ROC101.pdf))直观的比较了ROC和Precision-Recall在样本均衡性不同的情况下表现：

![1555402820805](C:\Users\jie\Documents\GitHub\jieyang1987.github.io\files\ROC_VS_PR.png)



可以看出，在正负样本比例为1:1和1:10时，ROC的曲线变化不是特别大，然而Precision-Recall曲线具有较大的变化。所以在衡量非均衡数据的时候，ROC更加合理。



